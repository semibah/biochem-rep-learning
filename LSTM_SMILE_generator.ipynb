{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tNHegtEhXbmz"
   },
   "outputs": [],
   "source": [
    "# If on google colab, uncomment the following lines\n",
    "# !wget -c https://repo.continuum.io/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "# !chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "# !time bash ./Miniconda3-latest-Linux-x86_64.sh -b -f -p /usr/local\n",
    "# !time conda install -q -y -c conda-forge rdkit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script features a simple LSTM that can autoregressively complete SMILES.\n",
    "\n",
    "# The purpose was to verify a simple model could learn the short-range dependencies that \n",
    "# valid SMILES require.\n",
    "\n",
    "# https://pytorch.org/tutorials/intermediate/char_rnn_generation_tutorial.html was referenced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kA9QRwBgcwuE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import random\n",
    "\n",
    "import string\n",
    "import time\n",
    "import math\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor\n",
    "from torch.autograd import Variable\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
    "from rdkit import Chem\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oHhd222zXerW"
   },
   "outputs": [],
   "source": [
    "all_characters = string.ascii_letters + string.punctuation + string.digits# abcd.... [\\]./, ....\n",
    "n_characters = len(all_characters) + 2 # + EOS + PAD. 96\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "MQfgEZRUXfXb",
    "outputId": "1bc14032-56c5-414d-9b6c-4eb6c565050c"
   },
   "outputs": [],
   "source": [
    "# Data is from bindingDB. it is the full list of SMILES. \n",
    "\n",
    "data = pd.read_csv('smile_data.csv')\n",
    "average_length = data['Ligand SMILES'].str.len().mean()\n",
    "data = data[data['Ligand SMILES'].map(lambda x: ' ' not in str(x))]\n",
    "len_data = len(data)\n",
    "print(\"Average length: {}\".format(average_length))\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dUuArCJ6Xffa"
   },
   "outputs": [],
   "source": [
    "# Function to generate human-readable time\n",
    "def time_since(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "# Generator for SMILE data. \n",
    "class smile_generator():\n",
    "    def __init__(self):\n",
    "        self.current_index = 0\n",
    "\n",
    "    def get_next_batch(self, batch_size, sequential):\n",
    "        if sequential is True:\n",
    "            if len_data - self.current_index <= batch_size:\n",
    "                self.current_index = 0\n",
    "            to_return = data['Ligand SMILES'].iloc[self.current_index:self.current_index+batch_size]\n",
    "            self.current_index += batch_size\n",
    "        else:\n",
    "            random_indice = np.random.randint(0, len_data-batch_size)\n",
    "            to_return = data['Ligand SMILES'].iloc[random_indice:random_indice+batch_size]\n",
    "        return list(to_return)\n",
    "\n",
    "    def target_smile(self, smile):\n",
    "        character_indices = [all_characters.find(smile[i]) for i in range(0, len(smile))]\n",
    "        character_indices.append(n_characters - 2) # we added 2 earlier for EOS and PAD, subtract 1 for EOS\n",
    "        return torch.LongTensor(character_indices)\n",
    "\n",
    "    def get_n_samples(self, n=32, sequential=False):\n",
    "        lst_targets = []\n",
    "        \n",
    "        if sequential == True:\n",
    "            temp_smile = self.get_next_batch(n)\n",
    "        else:\n",
    "            temp_smile = self.get_next_batch(n, sequential=False)\n",
    "\n",
    "        for i in range(n):\n",
    "            lst_targets.append(self.target_smile(temp_smile[i]))\n",
    "            \n",
    "        pad_token = n_characters - 1 # last character\n",
    "        lengths = [len(seq)-1 for seq in lst_targets]\n",
    "        max_len = max(lengths)\n",
    "        padded_Y = np.ones((n, max_len+1)) * pad_token\n",
    "        \n",
    "        for i, x_len in enumerate(lengths):\n",
    "            tgt = lst_targets[i]\n",
    "\n",
    "            padded_Y[i, 0:x_len+1] = tgt[:x_len+1]\n",
    "            \n",
    "        \n",
    "        return padded_Y, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvZn3qTIXflr"
   },
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "\n",
    "def sample(starting_token='C'):\n",
    "    with torch.no_grad():\n",
    "        inp = gen.target_smile(starting_token).to(device)\n",
    "        hidden = lstm.sample_init()\n",
    "        \n",
    "        output_seq = starting_token\n",
    "        \n",
    "        for i in range(max_len):\n",
    "            output, hidden = lstm(inp[0].unsqueeze(-1), 1, hidden)\n",
    "            output = output.squeeze(1)\n",
    "            topv, topi = output.topk(1)\n",
    "            topi = topi[0][0]\n",
    "            if topi == n_characters - 2:\n",
    "                break\n",
    "            elif topi == n_characters-1:\n",
    "                char = '<P>'\n",
    "                output_seq += char\n",
    "            else:\n",
    "                char = all_characters[topi]\n",
    "                output_seq += char\n",
    "            inp = gen.target_smile(char).to(device)\n",
    "            \n",
    "        unique_str = None\n",
    "        if Chem.MolFromSmiles(output_seq, sanitize=False) is not None:\n",
    "            prnt_str = \"The SMILE is valid\"\n",
    "            if output_seq in data['Ligand SMILES']:\n",
    "                unique_str = \"This SMILE is in the dataset\"\n",
    "            else:\n",
    "                unique_str = \"This SMILE is unique\"\n",
    "        else:\n",
    "            prnt_str = \"The SMILE is not valid\"\n",
    "            \n",
    "    return output_seq, prnt_str, unique_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SnClUZaOXfqz"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, hidden_size, batch_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding_dim = 256\n",
    "        self.n_layers = 1\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        \n",
    "        # takes in hidden units, outputs the prediction of char\n",
    "        self.fc1 = nn.Linear(self.hidden_size, n_characters)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "        self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings = n_characters,\n",
    "            embedding_dim = self.embedding_dim,\n",
    "            padding_idx = n_characters-1)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_dim,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.n_layers,\n",
    "            batch_first=True)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        # hidden weights, size = nlayers, batch, hidden\n",
    "        hidden_a = torch.randn(self.n_layers, self.batch_size, self.hidden_size)\n",
    "        hidden_b = torch.randn(self.n_layers, self.batch_size, self.hidden_size)\n",
    "\n",
    "        hidden_a = Variable(hidden_a).to(device)\n",
    "        hidden_b = Variable(hidden_b).to(device)\n",
    "        \n",
    "        return (hidden_a, hidden_b)\n",
    "    \n",
    "    def sample_init(self):\n",
    "        # hidden weights, size = nlayers, 1, hidden\n",
    "        hidden_a = torch.randn(self.n_layers, 1, self.hidden_size)\n",
    "        hidden_b = torch.randn(self.n_layers, 1, self.hidden_size)\n",
    "\n",
    "        hidden_a = Variable(hidden_a).to(device)\n",
    "        hidden_b = Variable(hidden_b).to(device)\n",
    "        \n",
    "        return (hidden_a, hidden_b)\n",
    "    \n",
    "    def forward(self, x, length, hidden):\n",
    "        x = x.unsqueeze(-1)\n",
    "        if x.size():\n",
    "            batch_size, seq_len = x.size()\n",
    "        else:\n",
    "            batch_size, seq_len = 1, 1\n",
    "        \n",
    "        x = self.embedding(x)\n",
    "        \n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "                \n",
    "        x = x.contiguous()\n",
    "        x = x.view(batch_size, -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        x = x.view(batch_size, seq_len, n_characters)\n",
    "        \n",
    "        return x, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IaigI0NfXfvO"
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "\n",
    "learning_rate = .005\n",
    "gen = smile_generator()\n",
    "def train():\n",
    "    y_train, length = gen.get_n_samples(n=128)\n",
    "    y_train, length = torch.LongTensor(y_train).to(device), torch.tensor(length).to(device)\n",
    "    hidden = lstm.init_hidden()\n",
    "\n",
    "    \n",
    "    lstm.zero_grad()\n",
    "    loss = 0\n",
    "\n",
    "    for i in range(len(y_train[0])-1):\n",
    "        output, hidden = lstm(y_train[:, i], length, hidden)\n",
    "        output = output.squeeze(1) \n",
    "        l = criterion(output, y_train[:, i+1])\n",
    "        loss += l\n",
    "        \n",
    "    loss.backward()\n",
    "\n",
    "    #if nans in loss, uncomment\n",
    "    #nn.utils.clip_grad_norm_(lstm.parameters(), 2)\n",
    "\n",
    "    for p in lstm.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "        \n",
    "    return loss.item() / y_train.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "03LfbWzyXfzT"
   },
   "outputs": [],
   "source": [
    "lstm = LSTM(1024, 128).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "zHTlYH_2Xf5n",
    "outputId": "bbe417a5-4ae6-4c1e-ddca-0bba8c4c3474"
   },
   "outputs": [],
   "source": [
    "n_iters = 10000\n",
    "print_every = 200\n",
    "plot_every = 500\n",
    "all_losses = []\n",
    "total_loss = 0 # Reset every plot_every iters\n",
    "\n",
    "start = time.time()\n",
    "for iter in range(1, n_iters + 1):\n",
    "    loss = train()\n",
    "    total_loss += loss\n",
    "    \n",
    "    if iter % print_every == 0:\n",
    "        print(\"Training time: {}. Iter: {}.\".format(time_since(start), iter))\n",
    "        print(\"Loss: {}\".format(loss))\n",
    "        SMILE, validity_str, unique_str = sample()\n",
    "        print(\"Randomly generated sample: {}\".format(SMILE))\n",
    "        print(validity_str)\n",
    "        if unique_str is not None:\n",
    "            print(unique_str)\n",
    "        \n",
    "\n",
    "    if iter % plot_every == 0:\n",
    "        all_losses.append(total_loss / plot_every)\n",
    "        total_loss = 0"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN_SMILE_generator.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
